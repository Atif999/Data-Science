{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]], dtype=int64),\n",
       " ['algorithm',\n",
       "  'and',\n",
       "  'category',\n",
       "  'clustering',\n",
       "  'comes',\n",
       "  'earth',\n",
       "  'errors',\n",
       "  'first',\n",
       "  'full',\n",
       "  'is',\n",
       "  'it',\n",
       "  'learning',\n",
       "  'my',\n",
       "  'of',\n",
       "  'program',\n",
       "  'round',\n",
       "  'second',\n",
       "  'the',\n",
       "  'this',\n",
       "  'under',\n",
       "  'unsupervised'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data=['this is my first program and it is full of errors', 'second algorithm',\n",
    "      'clustering comes under the category of unsupervised learning', \n",
    "      'the earth is round']\n",
    "vect = CountVectorizer()\n",
    "word_freq = vect.fit_transform(data)\n",
    "word_freq.toarray(), vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9162907318741551"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "(np.log(5/2)+1)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.91629073,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.91629073,  1.91629073,  1.91629073,  3.02165125,\n",
       "         1.91629073,  0.        ,  1.91629073,  1.51082562,  1.91629073,\n",
       "         0.        ,  0.        ,  0.        ,  1.91629073,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.91629073,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.91629073,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  1.91629073,  1.91629073,  1.91629073,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.91629073,  0.        ,  1.51082562,  0.        ,\n",
       "         0.        ,  0.        ,  1.51082562,  0.        ,  1.91629073,\n",
       "         1.91629073],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.91629073,  0.        ,  0.        ,  0.        ,  1.51082562,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.91629073,  0.        ,  1.51082562,  0.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(norm=None)\n",
    "tfidf = tfidf_transformer.fit_transform(word_freq.toarray())\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "#proide a word to be stemmed\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\n",
    "             \"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
      "python   are   veri   intellig   and   work   veri   pythonli   and   now   they   are   python   their   way   to   success   .  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    print(token_words)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "x=stemSentence(sentence)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Pythoners           Pythoners           \n",
      "are                 be                  \n",
      "very                very                \n",
      "intelligent         intelligent         \n",
      "and                 and                 \n",
      "work                work                \n",
      "very                very                \n",
      "pythonly            pythonly            \n",
      "and                 and                 \n",
      "now                 now                 \n",
      "they                they                \n",
      "are                 be                  \n",
      "pythoning           pythoning           \n",
      "their               their               \n",
      "way                 way                 \n",
      "to                  to                  \n",
      "success             success             \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "print(twenty_train.target_names) #prints all the categories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82381837493361654"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                            alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055894848645778011"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                         ('KMeans', KMeans(20)),])\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "ps=PorterStemmer()\n",
    "stemmed_dataset=[]\n",
    "for i in range(0,len(twenty_train.data)):\n",
    "    stemmed_array=twenty_train.data[i].split()\n",
    "    stemmed=[ps.stem(word) for word in stemmed_array if not word in set(stopwords.words('english'))]\n",
    "    stemmed=' '.join(stemmed)\n",
    "    stemmed_dataset.append(stemmed)\n",
    "print(stemmed_dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_test=[]\n",
    "for i in range(0,len(twenty_test.data)):\n",
    "    stemmed_array=twenty_test.data[i].split()\n",
    "    stemmed=[ps.stem(word) for word in stemmed_array if not word in set(stopwords.words('english'))]\n",
    "    stemmed=' '.join(stemmed)\n",
    "    stemmed_test.append(stemmed)\n",
    "print(stemmed_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                            alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "_ = text_clf_svm.fit(stemmed_dataset, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(stemmed_test)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3),}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3),}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "cv=CountVectorizer()\n",
    "wcss=[]\n",
    "X=cv.fit_transform(stemmed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(1,21):\n",
    "kmeans=KMeans(n_clusters=10,init='k-means++',max_iter=300,n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
